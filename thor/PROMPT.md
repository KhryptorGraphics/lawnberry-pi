# Task: LAWNBERRY AI TRAINING INFRASTRUCTURE - NVIDIA THOR

---

## RALPH ORCHESTRATOR META-INSTRUCTIONS

**YOU HAVE FULL AUTONOMY TO COMPLETE THIS PROJECT.**

### Dynamic Task Management
- **CREATE** new BEAD tasks when you discover additional work needed
- **EDIT** existing BEAD tasks to refine scope, update status, or clarify requirements
- **SPLIT** large tasks into smaller sub-tasks (e.g., BEAD-200a, BEAD-200b)
- **MERGE** tasks that are better handled together
- **REORDER** task priorities based on dependencies discovered during implementation
- **MARK COMPLETE** tasks as you finish them by changing `**Status**: pending` to `**Status**: complete`

### Error Recovery Strategy
When you encounter errors:
1. **Research First**: Use MCP web search tools (Perplexity, Tavily, Brave, Firecrawl) to find solutions
2. **Check JetPack/CUDA Compatibility**: Many issues stem from version mismatches
3. **Build from Source**: If pip packages fail, try building from source with correct CUDA version
4. **Document Workarounds**: Add notes to BEAD descriptions for future reference
5. **Create Fix Tasks**: If a fix requires significant work, create a new BEAD task

### Adaptive Iteration Strategy
1. **On Success**: Mark task complete, update this PROMPT.md, proceed to next task
2. **On Partial Success**: Document what worked, create follow-up tasks for remaining work
3. **On Failure**: Research the error, try alternative approaches, document findings
4. **On Blocker**: Document the blocker, skip to non-dependent tasks, return when unblocked

### Research-Driven Problem Solving
When encountering installation or compatibility issues:
```
1. Use mcp__perplexity-mcp__perplexity_search or mcp__tavily-mcp__tavily-search
2. Search for: "[package name] JetPack 7 CUDA 13 installation"
3. Check NVIDIA forums, GitHub issues, Stack Overflow
4. Try official NVIDIA containers if available
5. Build from source as last resort with correct CUDA paths
```

### Completion Criteria
**ITERATE UNTIL ALL OF THE FOLLOWING ARE TRUE:**
- [ ] Isaac Sim environment running and generating synthetic data
- [ ] Cosmos augmentation pipeline operational
- [ ] VLA model training on real + synthetic data
- [ ] World model trained and validated
- [ ] Model distillation to Hailo 8L format working
- [ ] SSH connectivity to LawnBerry Pi established
- [ ] Data sync from Pi to Thor operational

### Self-Modification Rules
1. Always update this PROMPT.md file after completing tasks
2. Add learnings and gotchas to relevant BEAD descriptions
3. Create new BEADs for unexpected dependencies or issues
4. Document any architectural decisions in the Notes section
5. Record successful installation commands for reproducibility

---

## System Information

### Hardware: NVIDIA Jetson AGX Thor
- **GPU**: Blackwell architecture, 2070 TFLOPS (FP4)
- **Memory**: 128GB unified RAM
- **Storage**: 8TB NVMe array
- **CUDA**: 13.x
- **JetPack**: 7.x
- **Network**: 10GbE + WiFi

### Network Configuration
- **Thor IP**: 192.168.1.64
- **LawnBerry Pi IP**: Will be discovered/configured
- **Pi User**: kp
- **Thor User**: kp

### Working Directory
```
/home/kp/mower/
├── config/
│   ├── training.yaml
│   ├── simulation.yaml
│   └── hardware.yaml
├── models/
│   ├── vla/
│   ├── world_model/
│   └── distilled/
├── data/
│   ├── real/           # Uploaded from Pi
│   ├── synthetic/      # Generated by Isaac Sim
│   └── augmented/      # Processed by Cosmos
├── scripts/
│   ├── train_vla.py
│   ├── train_world_model.py
│   ├── distill_hailo.py
│   └── sync_data.py
├── simulation/
│   └── lawn_env.py
└── logs/
```

---

## Environment Setup

### Pre-Flight Checks
```bash
# Verify JetPack version
cat /etc/nv_tegra_release

# Verify CUDA
nvcc --version
nvidia-smi

# Check available disk space (need ~500GB for datasets)
df -h /home/kp

# Check memory
free -h
```

### Python Environment
```bash
# Create virtual environment
cd /home/kp/mower
python3 -m venv .venv
source .venv/bin/activate

# Upgrade pip
pip install --upgrade pip setuptools wheel

# Install base packages
pip install numpy scipy matplotlib pandas tqdm pyyaml
```

---

## Beads Task List

### PHASE 2: NVIDIA THOR TRAINING INFRASTRUCTURE

---

### BEAD-200: Thor Server Base Setup
**Status**: pending
**Priority**: critical
**Description**: Configure Thor server environment for AI training

**Steps**:
1. Create directory structure
2. Set up Python virtual environment
3. Install PyTorch with CUDA 13 support
4. Verify GPU access from Python
5. Install base ML packages

```bash
# Directory structure
mkdir -p /home/kp/mower/{config,models/{vla,world_model,distilled},data/{real,synthetic,augmented},scripts,simulation,logs}

# Python environment
cd /home/kp/mower
python3 -m venv .venv
source .venv/bin/activate

# PyTorch for JetPack 7 / CUDA 13
# Check NVIDIA's PyTorch wheel for Jetson:
# https://developer.nvidia.com/embedded/downloads
# Or try:
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# If that fails, search for JetPack 7 compatible wheels:
# Use web search: "PyTorch JetPack 7 installation wheel"

# Verify CUDA access
python3 -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}'); print(f'Device: {torch.cuda.get_device_name(0)}')"
```

**Troubleshooting**:
- If PyTorch wheel not found, check NVIDIA's L4T PyTorch builds
- May need to build from source with CUDA 13 toolkit
- JetPack 7 may require nightly builds

**Acceptance**: `torch.cuda.is_available()` returns True

---

### BEAD-201: SSH Key Setup for Pi Access
**Status**: pending
**Priority**: critical
**Description**: Configure passwordless SSH from Thor to LawnBerry Pi

**Pi Connection Details**:
- Host: 192.168.1.64 (update if different)
- User: kp
- Password: terramax123 (for initial setup only)

**Steps**:
```bash
# Generate SSH key (if not exists)
[ -f ~/.ssh/id_ed25519 ] || ssh-keygen -t ed25519 -C "thor@lawnberry" -N "" -f ~/.ssh/id_ed25519

# Copy key to Pi (will prompt for password once)
ssh-copy-id -i ~/.ssh/id_ed25519.pub kp@192.168.1.64

# Test passwordless connection
ssh kp@192.168.1.64 "echo 'SSH connection successful!'; uname -a"

# Add to SSH config for convenience
cat >> ~/.ssh/config << 'EOF'
Host lawnberry pi
    HostName 192.168.1.64
    User kp
    IdentityFile ~/.ssh/id_ed25519
    StrictHostKeyChecking no
EOF

# Test using alias
ssh pi "hostname"
```

**Acceptance**: `ssh pi "echo success"` works without password prompt

---

### BEAD-202: Data Sync Service
**Status**: pending
**Priority**: high
**Description**: Set up automatic data sync from Pi to Thor

Create `scripts/sync_data.py`:
```python
"""Sync training data from LawnBerry Pi to Thor."""
import subprocess
import asyncio
import logging
from pathlib import Path
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

PI_HOST = "pi"  # Uses SSH config alias
PI_DATA_PATH = "/home/kp/repos/lawnberry_pi/data/recordings"
LOCAL_DATA_PATH = Path("/home/kp/mower/data/real")

async def sync_recordings():
    """Sync new recordings from Pi."""
    LOCAL_DATA_PATH.mkdir(parents=True, exist_ok=True)

    cmd = [
        "rsync", "-avz", "--progress",
        f"{PI_HOST}:{PI_DATA_PATH}/",
        str(LOCAL_DATA_PATH) + "/"
    ]

    logger.info(f"Syncing from {PI_HOST}...")
    proc = await asyncio.create_subprocess_exec(
        *cmd,
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE
    )
    stdout, stderr = await proc.communicate()

    if proc.returncode == 0:
        logger.info(f"Sync complete: {stdout.decode()}")
    else:
        logger.error(f"Sync failed: {stderr.decode()}")

    return proc.returncode == 0

async def watch_and_sync(interval_seconds: int = 300):
    """Continuously sync at interval."""
    while True:
        await sync_recordings()
        await asyncio.sleep(interval_seconds)

if __name__ == "__main__":
    asyncio.run(sync_recordings())
```

**Acceptance**: Running `python scripts/sync_data.py` successfully syncs data

---

### BEAD-203: Isaac Sim Installation
**Status**: pending
**Priority**: critical
**Description**: Install NVIDIA Isaac Sim for lawn simulation

**Note**: Isaac Sim installation on Jetson requires specific versions. Research current compatibility.

**Steps**:
```bash
# Check if Isaac Sim is available for JetPack 7
# Use web search: "Isaac Sim JetPack 7 Jetson AGX installation 2025"

# Option 1: NGC Container (recommended)
# Check available containers:
# https://catalog.ngc.nvidia.com/orgs/nvidia/containers/isaac-sim

# Option 2: Direct installation (if available)
# Follow NVIDIA's official guide

# Option 3: Isaac Lab (lighter alternative)
pip install isaacsim-rl isaacsim-robot

# Verify installation
python3 -c "from omni.isaac.core import World; print('Isaac Sim available')"
```

**Troubleshooting**:
- Isaac Sim may require Omniverse installation
- Check if headless mode is supported on Jetson
- May need to use Isaac Lab instead of full Isaac Sim
- Container approach often more reliable

**Acceptance**: Can import Isaac Sim core modules

---

### BEAD-204: Lawn Simulation Environment
**Status**: pending
**Priority**: critical
**Depends**: BEAD-203
**Description**: Create photorealistic lawn simulation

Create `simulation/lawn_env.py`:
```python
"""Isaac Sim lawn mowing environment for training."""
from __future__ import annotations

import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import yaml

# Isaac Sim imports (adjust based on installed version)
try:
    from omni.isaac.core import World
    from omni.isaac.core.prims import RigidPrim, GeometryPrim
    from omni.isaac.core.utils.stage import add_reference_to_stage
    from omni.isaac.sensor import Camera, IMUSensor
    ISAAC_AVAILABLE = True
except ImportError:
    ISAAC_AVAILABLE = False
    print("Warning: Isaac Sim not available, using simulation stubs")


class LawnSimEnvironment:
    """Photorealistic lawn simulation for VLA training.

    Generates synthetic training data matching real sensor configurations:
    - Stereo camera (ELP-960P equivalent)
    - Pi Camera RGB
    - GPS coordinates
    - IMU orientation
    - Ultrasonic distances
    """

    # Lawn grass types with visual properties
    LAWN_TYPES = {
        "kentucky_bluegrass": {"color": (0.2, 0.5, 0.15), "height_cm": 8},
        "bermuda": {"color": (0.25, 0.55, 0.12), "height_cm": 5},
        "fescue": {"color": (0.18, 0.45, 0.1), "height_cm": 10},
        "zoysia": {"color": (0.22, 0.48, 0.14), "height_cm": 6},
    }

    # Obstacle definitions
    OBSTACLE_TYPES = ["tree", "rock", "flower_bed", "fence", "garden_gnome", "pet_toy"]

    # Weather/lighting conditions
    WEATHER_CONDITIONS = ["sunny", "cloudy", "morning_dew", "afternoon_heat", "overcast"]

    def __init__(self, config_path: Optional[str] = None):
        """Initialize lawn simulation environment."""
        self.config = self._load_config(config_path)

        if ISAAC_AVAILABLE:
            self.world = World(stage_units_in_meters=1.0)
        else:
            self.world = None

        self.mower_prim = None
        self.cameras = {}
        self.sensors = {}

    def _load_config(self, config_path: Optional[str]) -> dict:
        """Load simulation configuration."""
        default_config = {
            "lawn_size_m": [20, 30],  # width x length
            "mower_start_pos": [10, 5, 0.1],
            "camera_resolution": [960, 540],
            "physics_dt": 1/60,
            "render_dt": 1/30,
        }

        if config_path and Path(config_path).exists():
            with open(config_path) as f:
                user_config = yaml.safe_load(f)
                default_config.update(user_config)

        return default_config

    def create_lawn_scene(
        self,
        lawn_type: str = "kentucky_bluegrass",
        size_m: Tuple[float, float] = (20, 30),
        obstacles: Optional[List[dict]] = None,
        weather: str = "sunny"
    ) -> None:
        """Generate a randomized lawn environment.

        Args:
            lawn_type: Type of grass (affects color/texture)
            size_m: Lawn dimensions (width, length) in meters
            obstacles: List of obstacle definitions
            weather: Weather condition for lighting
        """
        if not ISAAC_AVAILABLE:
            print(f"[SIM] Creating lawn: {lawn_type}, {size_m}m, weather={weather}")
            return

        # Create ground plane with grass texture
        grass_props = self.LAWN_TYPES.get(lawn_type, self.LAWN_TYPES["kentucky_bluegrass"])

        # Add terrain
        # ... Isaac Sim terrain creation code

        # Add obstacles
        if obstacles:
            for obs in obstacles:
                self._add_obstacle(obs)

        # Configure lighting for weather
        self._set_weather_lighting(weather)

    def spawn_mower(self, position: Tuple[float, float, float] = (10, 5, 0.1)) -> None:
        """Spawn the LawnBerry Pi mower model.

        Args:
            position: (x, y, z) spawn position in meters
        """
        if not ISAAC_AVAILABLE:
            print(f"[SIM] Spawning mower at {position}")
            return

        # Load mower USD/URDF model
        # Attach simulated sensors matching real hardware
        pass

    def get_sensor_data(self) -> Dict[str, np.ndarray]:
        """Get simulated sensor readings matching real hardware.

        Returns:
            Dictionary with sensor data matching MowerDataFrame structure
        """
        if not ISAAC_AVAILABLE:
            # Return dummy data for testing
            return {
                "stereo_left": np.random.randint(0, 255, (540, 960, 3), dtype=np.uint8),
                "stereo_right": np.random.randint(0, 255, (540, 960, 3), dtype=np.uint8),
                "pi_camera": np.random.randint(0, 255, (720, 1280, 3), dtype=np.uint8),
                "gps": {"latitude": 40.7128, "longitude": -74.0060, "altitude": 10.0},
                "imu": {"roll": 0.0, "pitch": 0.0, "yaw": np.random.uniform(0, 360)},
                "ultrasonic": {"front_left": 100, "front_center": 150, "front_right": 120},
            }

        # Real Isaac Sim sensor capture
        return {
            "stereo_left": self._render_camera("stereo_left"),
            "stereo_right": self._render_camera("stereo_right"),
            "pi_camera": self._render_camera("pi_camera"),
            "gps": self._get_simulated_gps(),
            "imu": self._get_simulated_imu(),
            "ultrasonic": self._get_simulated_ultrasonic(),
        }

    def step(self, action: Dict[str, float]) -> Tuple[dict, float, bool, dict]:
        """Execute action and return (observation, reward, done, info).

        Args:
            action: {"steering": -1 to 1, "throttle": 0 to 1, "blade": bool}

        Returns:
            Tuple of (observation, reward, done, info)
        """
        if not ISAAC_AVAILABLE:
            obs = self.get_sensor_data()
            reward = 0.0
            done = False
            info = {"simulated": True}
            return obs, reward, done, info

        # Apply motor commands to mower
        # Step physics
        # Calculate reward based on:
        # - Area covered
        # - Obstacle avoidance
        # - Efficiency
        # - Safety violations
        pass

    def reset(self, randomize: bool = True) -> dict:
        """Reset environment to initial state.

        Args:
            randomize: If True, randomize lawn type, obstacles, weather

        Returns:
            Initial observation
        """
        if randomize:
            lawn_type = np.random.choice(list(self.LAWN_TYPES.keys()))
            weather = np.random.choice(self.WEATHER_CONDITIONS)
            num_obstacles = np.random.randint(3, 10)
            obstacles = self._generate_random_obstacles(num_obstacles)
        else:
            lawn_type = "kentucky_bluegrass"
            weather = "sunny"
            obstacles = []

        self.create_lawn_scene(lawn_type, obstacles=obstacles, weather=weather)
        self.spawn_mower()

        return self.get_sensor_data()

    def _generate_random_obstacles(self, count: int) -> List[dict]:
        """Generate random obstacle placements."""
        obstacles = []
        for _ in range(count):
            obstacles.append({
                "type": np.random.choice(self.OBSTACLE_TYPES),
                "position": [
                    np.random.uniform(2, 18),
                    np.random.uniform(2, 28),
                    0
                ],
                "rotation": np.random.uniform(0, 360),
                "scale": np.random.uniform(0.8, 1.2),
            })
        return obstacles


# Entry point for testing
if __name__ == "__main__":
    env = LawnSimEnvironment()
    obs = env.reset()
    print(f"Observation keys: {obs.keys()}")
    print(f"Stereo left shape: {obs['stereo_left'].shape}")
```

**Acceptance**: Environment generates synthetic sensor data matching real hardware format

---

### BEAD-205: Cosmos Installation
**Status**: pending
**Priority**: high
**Description**: Install NVIDIA Cosmos for synthetic data augmentation

```bash
# Research current Cosmos availability for JetPack 7
# Use web search: "NVIDIA Cosmos installation JetPack 7 2025"

# Option 1: pip install (if available)
pip install nvidia-cosmos

# Option 2: NGC Container
# docker pull nvcr.io/nvidia/cosmos:latest

# Option 3: Build from source
git clone https://github.com/NVIDIA/Cosmos.git
cd Cosmos
pip install -e .

# Verify
python3 -c "import cosmos; print('Cosmos available')"
```

**Troubleshooting**:
- Cosmos may have specific CUDA requirements
- Check NGC catalog for compatible containers
- May need to use Cosmos components individually

**Acceptance**: Can import and use Cosmos augmentation functions

---

### BEAD-206: Cosmos Data Augmentation Pipeline
**Status**: pending
**Priority**: high
**Depends**: BEAD-205
**Description**: Create data augmentation pipeline using Cosmos

Create `scripts/augment_data.py`:
```python
"""Augment training data using NVIDIA Cosmos."""
from __future__ import annotations

import asyncio
import logging
from pathlib import Path
from typing import List, Dict, Any
import numpy as np

try:
    import cosmos
    from cosmos import augmentation as aug
    COSMOS_AVAILABLE = True
except ImportError:
    COSMOS_AVAILABLE = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class CosmosAugmentor:
    """Augment lawn mowing data with synthetic variations."""

    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}

        if COSMOS_AVAILABLE:
            self._init_cosmos()
        else:
            logger.warning("Cosmos not available, using basic augmentations")

    def _init_cosmos(self):
        """Initialize Cosmos augmentation pipeline."""
        # Configure augmentation parameters
        pass

    def augment_frame(self, frame_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Generate augmented versions of a single frame.

        Augmentations:
        - Lighting variations (time of day, weather)
        - Grass appearance (wet, dry, freshly cut)
        - Camera noise/blur
        - Obstacle appearance variations

        Args:
            frame_data: Original frame data dict

        Returns:
            List of augmented frame variants
        """
        augmented = []

        # Weather variations
        for weather in ["sunny", "cloudy", "overcast"]:
            aug_frame = self._apply_weather(frame_data.copy(), weather)
            augmented.append(aug_frame)

        # Lighting variations
        for time_of_day in ["morning", "noon", "afternoon", "evening"]:
            aug_frame = self._apply_lighting(frame_data.copy(), time_of_day)
            augmented.append(aug_frame)

        # Noise/blur variations
        for noise_level in [0.01, 0.02, 0.05]:
            aug_frame = self._apply_noise(frame_data.copy(), noise_level)
            augmented.append(aug_frame)

        return augmented

    def _apply_weather(self, frame: dict, weather: str) -> dict:
        """Apply weather-based augmentation."""
        if COSMOS_AVAILABLE:
            # Use Cosmos weather augmentation
            pass
        else:
            # Basic color adjustment
            if "stereo_left" in frame and frame["stereo_left"] is not None:
                if weather == "cloudy":
                    frame["stereo_left"] = (frame["stereo_left"] * 0.8).astype(np.uint8)
                elif weather == "overcast":
                    frame["stereo_left"] = (frame["stereo_left"] * 0.6).astype(np.uint8)
        return frame

    def _apply_lighting(self, frame: dict, time_of_day: str) -> dict:
        """Apply time-of-day lighting augmentation."""
        # Adjust brightness/color temperature based on time
        return frame

    def _apply_noise(self, frame: dict, noise_level: float) -> dict:
        """Apply camera noise augmentation."""
        if "stereo_left" in frame and frame["stereo_left"] is not None:
            noise = np.random.normal(0, noise_level * 255, frame["stereo_left"].shape)
            frame["stereo_left"] = np.clip(frame["stereo_left"] + noise, 0, 255).astype(np.uint8)
        return frame


async def process_dataset(input_dir: Path, output_dir: Path):
    """Process entire dataset with augmentation."""
    output_dir.mkdir(parents=True, exist_ok=True)
    augmentor = CosmosAugmentor()

    # Find all recording files
    recordings = list(input_dir.glob("*.msgpack"))
    logger.info(f"Found {len(recordings)} recordings to augment")

    for recording_path in recordings:
        logger.info(f"Processing {recording_path.name}")
        # Load, augment, save
        # ...


if __name__ == "__main__":
    input_dir = Path("/home/kp/mower/data/real")
    output_dir = Path("/home/kp/mower/data/augmented")
    asyncio.run(process_dataset(input_dir, output_dir))
```

**Acceptance**: Augmentation pipeline generates diverse training variations

---

### BEAD-207: VLA Model Architecture
**Status**: pending
**Priority**: critical
**Depends**: BEAD-200
**Description**: Implement Vision-Language-Action model for lawn mowing

Create `models/vla/lawn_vla.py`:
```python
"""LawnMower Vision-Language-Action Model."""
from __future__ import annotations

import torch
import torch.nn as nn
from typing import Dict, Tuple, Optional
import logging

logger = logging.getLogger(__name__)


class LawnMowerVLA(nn.Module):
    """Vision-Language-Action model for autonomous lawn mowing.

    Architecture:
    - Visual encoder: ResNet50 or ViT for stereo + RGB images
    - Sensor encoder: MLP for GPS, IMU, ultrasonic data
    - Fusion network: Transformer for multi-modal fusion
    - Action head: MLP predicting steering, throttle, blade

    Input modalities:
    - stereo_left: (B, 3, 540, 960) - Left camera image
    - stereo_right: (B, 3, 540, 960) - Right camera image
    - pi_camera: (B, 3, 720, 1280) - High-res RGB image
    - gps: (B, 4) - lat, lon, heading, speed
    - imu: (B, 9) - roll, pitch, yaw, accel_xyz, gyro_xyz
    - ultrasonic: (B, 3) - front left, center, right distances

    Output:
    - steering: (B, 1) - Range [-1, 1]
    - throttle: (B, 1) - Range [0, 1]
    - blade: (B, 1) - Probability [0, 1]
    """

    def __init__(
        self,
        visual_encoder: str = "resnet50",
        hidden_dim: int = 512,
        num_transformer_layers: int = 4,
        num_heads: int = 8,
        dropout: float = 0.1,
    ):
        super().__init__()

        self.hidden_dim = hidden_dim

        # Visual encoders (shared weights for stereo pair)
        if visual_encoder == "resnet50":
            from torchvision.models import resnet50, ResNet50_Weights
            backbone = resnet50(weights=ResNet50_Weights.DEFAULT)
            self.visual_encoder = nn.Sequential(*list(backbone.children())[:-1])
            visual_out_dim = 2048
        else:
            raise ValueError(f"Unknown visual encoder: {visual_encoder}")

        # Project visual features to hidden dim
        self.visual_proj = nn.Linear(visual_out_dim, hidden_dim)

        # Sensor encoders
        self.gps_encoder = nn.Sequential(
            nn.Linear(4, 64),
            nn.ReLU(),
            nn.Linear(64, hidden_dim),
        )

        self.imu_encoder = nn.Sequential(
            nn.Linear(9, 64),
            nn.ReLU(),
            nn.Linear(64, hidden_dim),
        )

        self.ultrasonic_encoder = nn.Sequential(
            nn.Linear(3, 32),
            nn.ReLU(),
            nn.Linear(32, hidden_dim),
        )

        # Multi-modal fusion transformer
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim,
            nhead=num_heads,
            dim_feedforward=hidden_dim * 4,
            dropout=dropout,
            batch_first=True,
        )
        self.fusion_transformer = nn.TransformerEncoder(
            encoder_layer,
            num_layers=num_transformer_layers,
        )

        # Learnable modality tokens
        self.modality_embeddings = nn.Parameter(torch.randn(6, hidden_dim))

        # Action prediction heads
        self.steering_head = nn.Sequential(
            nn.Linear(hidden_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 1),
            nn.Tanh(),  # Output [-1, 1]
        )

        self.throttle_head = nn.Sequential(
            nn.Linear(hidden_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 1),
            nn.Sigmoid(),  # Output [0, 1]
        )

        self.blade_head = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid(),  # Output [0, 1] probability
        )

    def encode_visual(self, image: torch.Tensor) -> torch.Tensor:
        """Encode visual input to feature vector."""
        # image: (B, 3, H, W)
        features = self.visual_encoder(image)  # (B, 2048, 1, 1)
        features = features.flatten(1)  # (B, 2048)
        return self.visual_proj(features)  # (B, hidden_dim)

    def forward(
        self,
        stereo_left: torch.Tensor,
        stereo_right: torch.Tensor,
        pi_camera: torch.Tensor,
        gps: torch.Tensor,
        imu: torch.Tensor,
        ultrasonic: torch.Tensor,
    ) -> Dict[str, torch.Tensor]:
        """Forward pass.

        Args:
            stereo_left: (B, 3, 540, 960)
            stereo_right: (B, 3, 540, 960)
            pi_camera: (B, 3, 720, 1280)
            gps: (B, 4)
            imu: (B, 9)
            ultrasonic: (B, 3)

        Returns:
            Dict with steering, throttle, blade predictions
        """
        batch_size = stereo_left.size(0)

        # Encode all modalities
        stereo_l_feat = self.encode_visual(stereo_left)  # (B, hidden_dim)
        stereo_r_feat = self.encode_visual(stereo_right)
        pi_cam_feat = self.encode_visual(pi_camera)
        gps_feat = self.gps_encoder(gps)
        imu_feat = self.imu_encoder(imu)
        ultra_feat = self.ultrasonic_encoder(ultrasonic)

        # Stack features and add modality embeddings
        features = torch.stack([
            stereo_l_feat, stereo_r_feat, pi_cam_feat,
            gps_feat, imu_feat, ultra_feat
        ], dim=1)  # (B, 6, hidden_dim)

        features = features + self.modality_embeddings.unsqueeze(0)

        # Fusion transformer
        fused = self.fusion_transformer(features)  # (B, 6, hidden_dim)

        # Global pooling
        pooled = fused.mean(dim=1)  # (B, hidden_dim)

        # Predict actions
        steering = self.steering_head(pooled)
        throttle = self.throttle_head(pooled)
        blade = self.blade_head(pooled)

        return {
            "steering": steering,
            "throttle": throttle,
            "blade": blade,
        }

    def get_action(
        self,
        stereo_left: torch.Tensor,
        stereo_right: torch.Tensor,
        pi_camera: torch.Tensor,
        gps: torch.Tensor,
        imu: torch.Tensor,
        ultrasonic: torch.Tensor,
    ) -> Dict[str, float]:
        """Get action for deployment (single sample, no grad)."""
        self.eval()
        with torch.no_grad():
            outputs = self.forward(
                stereo_left.unsqueeze(0),
                stereo_right.unsqueeze(0),
                pi_camera.unsqueeze(0),
                gps.unsqueeze(0),
                imu.unsqueeze(0),
                ultrasonic.unsqueeze(0),
            )

        return {
            "steering": outputs["steering"].item(),
            "throttle": outputs["throttle"].item(),
            "blade": outputs["blade"].item() > 0.5,
        }


def create_lawn_vla(pretrained: bool = False) -> LawnMowerVLA:
    """Factory function to create LawnMowerVLA model."""
    model = LawnMowerVLA()

    if pretrained:
        # Load pretrained weights if available
        weights_path = "/home/kp/mower/models/vla/lawn_vla_pretrained.pt"
        if Path(weights_path).exists():
            model.load_state_dict(torch.load(weights_path))
            logger.info(f"Loaded pretrained weights from {weights_path}")

    return model


if __name__ == "__main__":
    # Test model
    model = create_lawn_vla()
    print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")

    # Test forward pass
    batch = {
        "stereo_left": torch.randn(2, 3, 540, 960),
        "stereo_right": torch.randn(2, 3, 540, 960),
        "pi_camera": torch.randn(2, 3, 720, 1280),
        "gps": torch.randn(2, 4),
        "imu": torch.randn(2, 9),
        "ultrasonic": torch.randn(2, 3),
    }

    outputs = model(**batch)
    print(f"Steering shape: {outputs['steering'].shape}")
    print(f"Throttle shape: {outputs['throttle'].shape}")
    print(f"Blade shape: {outputs['blade'].shape}")
```

**Acceptance**: Model forward pass works with correct input/output shapes

---

### BEAD-208: VLA Training Script
**Status**: pending
**Priority**: critical
**Depends**: BEAD-207, BEAD-202
**Description**: Create training script for VLA model

Create `scripts/train_vla.py`:
```python
"""Train LawnMower VLA model."""
from __future__ import annotations

import argparse
import logging
from pathlib import Path
from datetime import datetime
import yaml

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
from tqdm import tqdm

# Add parent to path for imports
import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

from models.vla.lawn_vla import create_lawn_vla

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MowerDataset(Dataset):
    """Dataset for mower training data."""

    def __init__(self, data_dir: Path, transform=None):
        self.data_dir = data_dir
        self.transform = transform
        self.samples = self._load_samples()

    def _load_samples(self):
        """Load sample paths from data directory."""
        samples = []
        for path in self.data_dir.glob("**/*.msgpack"):
            samples.append(path)
        return samples

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        # Load and preprocess sample
        # Return dict with all modalities and labels
        pass


def train_epoch(model, dataloader, optimizer, criterion, device):
    """Train for one epoch."""
    model.train()
    total_loss = 0

    for batch in tqdm(dataloader, desc="Training"):
        # Move to device
        for key in batch:
            if isinstance(batch[key], torch.Tensor):
                batch[key] = batch[key].to(device)

        # Forward pass
        outputs = model(
            stereo_left=batch["stereo_left"],
            stereo_right=batch["stereo_right"],
            pi_camera=batch["pi_camera"],
            gps=batch["gps"],
            imu=batch["imu"],
            ultrasonic=batch["ultrasonic"],
        )

        # Calculate loss
        loss = (
            criterion(outputs["steering"], batch["steering_label"]) +
            criterion(outputs["throttle"], batch["throttle_label"]) +
            nn.BCELoss()(outputs["blade"], batch["blade_label"].float())
        )

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    return total_loss / len(dataloader)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, default="config/training.yaml")
    parser.add_argument("--data-dir", type=str, default="data/augmented")
    parser.add_argument("--output-dir", type=str, default="models/vla")
    parser.add_argument("--epochs", type=int, default=100)
    parser.add_argument("--batch-size", type=int, default=8)
    parser.add_argument("--lr", type=float, default=1e-4)
    args = parser.parse_args()

    # Setup
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"Using device: {device}")

    # Create model
    model = create_lawn_vla().to(device)

    # Create dataset and dataloader
    dataset = MowerDataset(Path(args.data_dir))
    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=4)

    # Optimizer and scheduler
    optimizer = AdamW(model.parameters(), lr=args.lr)
    scheduler = CosineAnnealingLR(optimizer, T_max=args.epochs)
    criterion = nn.MSELoss()

    # Training loop
    best_loss = float("inf")
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    for epoch in range(args.epochs):
        loss = train_epoch(model, dataloader, optimizer, criterion, device)
        scheduler.step()

        logger.info(f"Epoch {epoch+1}/{args.epochs} - Loss: {loss:.4f}")

        # Save best model
        if loss < best_loss:
            best_loss = loss
            torch.save(model.state_dict(), output_dir / "lawn_vla_best.pt")

        # Save checkpoint
        if (epoch + 1) % 10 == 0:
            torch.save({
                "epoch": epoch,
                "model_state_dict": model.state_dict(),
                "optimizer_state_dict": optimizer.state_dict(),
                "loss": loss,
            }, output_dir / f"checkpoint_epoch_{epoch+1}.pt")

    logger.info(f"Training complete. Best loss: {best_loss:.4f}")


if __name__ == "__main__":
    main()
```

**Acceptance**: Training runs without errors, loss decreases

---

### BEAD-209: World Model Implementation
**Status**: pending
**Priority**: high
**Depends**: BEAD-200
**Description**: Implement world model for simulation and planning

Create `models/world_model/lawn_world_model.py`:
```python
"""World Model for lawn mowing environment prediction."""
from __future__ import annotations

import torch
import torch.nn as nn
from typing import Dict, Tuple


class LawnWorldModel(nn.Module):
    """Predicts future states given current state and action.

    Used for:
    - Imagination-based planning
    - Safety validation (predict if action leads to collision)
    - Data augmentation (generate rollouts)

    Architecture:
    - State encoder (same as VLA visual encoder)
    - Action encoder (MLP)
    - Dynamics model (LSTM or Transformer)
    - State decoder (reconstruct predicted state)
    """

    def __init__(self, hidden_dim: int = 512, horizon: int = 10):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.horizon = horizon

        # State encoder (simplified)
        self.state_encoder = nn.Sequential(
            nn.Linear(1024, hidden_dim),  # Placeholder for visual features
            nn.ReLU(),
        )

        # Action encoder
        self.action_encoder = nn.Sequential(
            nn.Linear(3, 64),  # steering, throttle, blade
            nn.ReLU(),
            nn.Linear(64, hidden_dim),
        )

        # Dynamics model
        self.dynamics = nn.LSTM(
            input_size=hidden_dim * 2,
            hidden_size=hidden_dim,
            num_layers=2,
            batch_first=True,
        )

        # Prediction heads
        self.position_head = nn.Linear(hidden_dim, 3)  # x, y, heading
        self.collision_head = nn.Linear(hidden_dim, 1)  # collision probability

    def forward(
        self,
        state: torch.Tensor,
        actions: torch.Tensor,
    ) -> Dict[str, torch.Tensor]:
        """Predict future states.

        Args:
            state: Current state encoding (B, state_dim)
            actions: Sequence of actions (B, T, 3)

        Returns:
            Predicted positions and collision probabilities
        """
        batch_size, seq_len, _ = actions.shape

        state_enc = self.state_encoder(state)  # (B, hidden_dim)

        # Encode action sequence
        action_enc = self.action_encoder(actions)  # (B, T, hidden_dim)

        # Combine state and actions
        state_expanded = state_enc.unsqueeze(1).expand(-1, seq_len, -1)
        combined = torch.cat([state_expanded, action_enc], dim=-1)

        # Dynamics prediction
        dynamics_out, _ = self.dynamics(combined)  # (B, T, hidden_dim)

        # Predict positions and collisions
        positions = self.position_head(dynamics_out)  # (B, T, 3)
        collisions = torch.sigmoid(self.collision_head(dynamics_out))  # (B, T, 1)

        return {
            "positions": positions,
            "collision_probs": collisions,
        }

    def imagine_rollout(
        self,
        initial_state: torch.Tensor,
        policy: nn.Module,
        horizon: int = None,
    ) -> Dict[str, torch.Tensor]:
        """Generate imagined rollout using policy.

        Useful for planning and data augmentation.
        """
        horizon = horizon or self.horizon
        # Implementation for imagination-based planning
        pass


if __name__ == "__main__":
    # Test world model
    model = LawnWorldModel()
    state = torch.randn(4, 1024)
    actions = torch.randn(4, 10, 3)

    outputs = model(state, actions)
    print(f"Positions shape: {outputs['positions'].shape}")
    print(f"Collision probs shape: {outputs['collision_probs'].shape}")
```

**Acceptance**: World model predicts reasonable future states

---

### BEAD-210: Model Distillation to Hailo
**Status**: pending
**Priority**: critical
**Depends**: BEAD-208
**Description**: Distill trained model to Hailo 8L compatible format

Create `scripts/distill_hailo.py`:
```python
"""Distill VLA model to Hailo 8L HEF format."""
from __future__ import annotations

import argparse
import logging
from pathlib import Path

import torch
import torch.nn as nn
import onnx
import onnxruntime

# Add parent to path
import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

from models.vla.lawn_vla import create_lawn_vla

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DistilledLawnVLA(nn.Module):
    """Smaller model for edge deployment on Hailo 8L.

    Optimizations:
    - Reduced visual encoder (MobileNetV3 instead of ResNet50)
    - Quantization-friendly architecture
    - Single image input (stereo fusion done in preprocessing)
    """

    def __init__(self, hidden_dim: int = 256):
        super().__init__()

        from torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights

        backbone = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.DEFAULT)
        self.visual_encoder = nn.Sequential(*list(backbone.children())[:-1])

        self.fusion = nn.Sequential(
            nn.Linear(576 + 16, hidden_dim),  # visual + sensors
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
        )

        self.action_head = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 3),  # steering, throttle, blade
        )

    def forward(self, image: torch.Tensor, sensors: torch.Tensor) -> torch.Tensor:
        """Forward pass for edge inference.

        Args:
            image: Fused stereo/rgb image (B, 3, 224, 224)
            sensors: Concatenated sensor data (B, 16)

        Returns:
            Actions (B, 3) - steering, throttle, blade_prob
        """
        visual = self.visual_encoder(image).flatten(1)
        combined = torch.cat([visual, sensors], dim=-1)
        features = self.fusion(combined)
        actions = self.action_head(features)

        # Apply activations
        actions[:, 0] = torch.tanh(actions[:, 0])  # steering [-1, 1]
        actions[:, 1] = torch.sigmoid(actions[:, 1])  # throttle [0, 1]
        actions[:, 2] = torch.sigmoid(actions[:, 2])  # blade [0, 1]

        return actions


def distill_model(teacher_path: str, output_dir: str):
    """Distill teacher model to student."""
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # Load teacher
    teacher = create_lawn_vla()
    teacher.load_state_dict(torch.load(teacher_path))
    teacher.eval()

    # Create student
    student = DistilledLawnVLA()

    # Knowledge distillation training
    # ... (training loop with teacher outputs as soft labels)

    # Save PyTorch model
    torch.save(student.state_dict(), output_dir / "distilled_vla.pt")

    # Export to ONNX
    dummy_image = torch.randn(1, 3, 224, 224)
    dummy_sensors = torch.randn(1, 16)

    onnx_path = output_dir / "distilled_vla.onnx"
    torch.onnx.export(
        student,
        (dummy_image, dummy_sensors),
        onnx_path,
        input_names=["image", "sensors"],
        output_names=["actions"],
        dynamic_axes={
            "image": {0: "batch"},
            "sensors": {0: "batch"},
            "actions": {0: "batch"},
        },
        opset_version=13,
    )
    logger.info(f"Exported ONNX model to {onnx_path}")

    # Verify ONNX model
    onnx_model = onnx.load(onnx_path)
    onnx.checker.check_model(onnx_model)
    logger.info("ONNX model validation passed")

    # Test with ONNX Runtime
    session = onnxruntime.InferenceSession(str(onnx_path))
    outputs = session.run(
        None,
        {"image": dummy_image.numpy(), "sensors": dummy_sensors.numpy()}
    )
    logger.info(f"ONNX inference test: output shape {outputs[0].shape}")

    return onnx_path


def convert_to_hailo(onnx_path: str, output_path: str):
    """Convert ONNX to Hailo HEF format.

    Requires Hailo Dataflow Compiler (hailo_sdk_client).
    """
    try:
        from hailo_sdk_client import ClientRunner

        # Create runner for Hailo-8L
        runner = ClientRunner(hw_arch="hailo8l")

        # Load ONNX model
        runner.translate_onnx_model(onnx_path)

        # Optimize for INT8
        runner.optimize_model()

        # Compile to HEF
        runner.compile()

        # Save HEF file
        runner.save_har(output_path)

        logger.info(f"Compiled HEF model to {output_path}")

    except ImportError:
        logger.warning("Hailo SDK not available. ONNX model saved, convert on Pi.")
        logger.info("Transfer ONNX to Pi and use: hailo_sdk_client to compile")


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--teacher", type=str, default="models/vla/lawn_vla_best.pt")
    parser.add_argument("--output", type=str, default="models/distilled")
    args = parser.parse_args()

    onnx_path = distill_model(args.teacher, args.output)
    convert_to_hailo(str(onnx_path), f"{args.output}/lawn_vla.hef")


if __name__ == "__main__":
    main()
```

**Acceptance**: HEF file generated that can be loaded on Hailo 8L

---

### BEAD-211: Training Configuration
**Status**: pending
**Priority**: medium
**Description**: Create configuration files for training

Create `config/training.yaml`:
```yaml
# LawnBerry AI Training Configuration

# Model Architecture
model:
  type: "lawn_vla"
  visual_encoder: "resnet50"
  hidden_dim: 512
  num_transformer_layers: 4
  num_heads: 8
  dropout: 0.1

# Training Hyperparameters
training:
  epochs: 100
  batch_size: 8
  learning_rate: 1.0e-4
  weight_decay: 0.01
  warmup_epochs: 5

  # Loss weights
  steering_weight: 1.0
  throttle_weight: 1.0
  blade_weight: 0.5

  # Data augmentation
  augmentation:
    enabled: true
    horizontal_flip: 0.5
    color_jitter: 0.2
    gaussian_noise: 0.01

# Dataset
data:
  real_data_dir: "/home/kp/mower/data/real"
  synthetic_data_dir: "/home/kp/mower/data/synthetic"
  augmented_data_dir: "/home/kp/mower/data/augmented"

  # Mix ratios
  real_ratio: 0.4
  synthetic_ratio: 0.3
  augmented_ratio: 0.3

  # Preprocessing
  image_size: [224, 224]  # For distilled model
  normalize_mean: [0.485, 0.456, 0.406]
  normalize_std: [0.229, 0.224, 0.225]

# Distillation
distillation:
  enabled: true
  temperature: 3.0
  alpha: 0.5  # Weight for distillation loss vs hard labels

  student:
    type: "distilled_lawn_vla"
    hidden_dim: 256

# Hardware
hardware:
  device: "cuda"
  num_workers: 4
  pin_memory: true
  mixed_precision: true

# Checkpointing
checkpoint:
  save_dir: "/home/kp/mower/models/vla"
  save_frequency: 10  # epochs
  keep_last_n: 5

# Logging
logging:
  tensorboard: true
  wandb: false
  log_frequency: 100  # steps
```

Create `config/simulation.yaml`:
```yaml
# Isaac Sim Lawn Environment Configuration

environment:
  # Lawn parameters
  default_lawn_type: "kentucky_bluegrass"
  lawn_size_m: [20, 30]

  # Physics
  physics_dt: 0.016667  # 60 Hz
  render_dt: 0.033333   # 30 Hz

  # Randomization
  randomize_lawn: true
  randomize_obstacles: true
  randomize_weather: true

  # Obstacle generation
  obstacles:
    min_count: 3
    max_count: 10
    types: ["tree", "rock", "flower_bed", "fence", "garden_gnome"]

  # Weather conditions
  weather_conditions: ["sunny", "cloudy", "morning_dew", "afternoon_heat", "overcast"]

# Mower simulation
mower:
  # Physical properties
  mass_kg: 15.0
  wheel_base_m: 0.4
  wheel_radius_m: 0.1
  max_speed_mps: 1.0

  # Sensors (match real hardware)
  cameras:
    stereo:
      resolution: [960, 540]
      fov_deg: 90
      baseline_m: 0.06
    pi_camera:
      resolution: [1280, 720]
      fov_deg: 62

  ultrasonic:
    positions: [[-0.15, 0.1, 0.1], [0, 0.15, 0.1], [0.15, 0.1, 0.1]]
    range_m: 4.0

# Data generation
data_generation:
  episodes_per_lawn: 100
  max_steps_per_episode: 1000
  save_frequency: 10  # steps
  output_dir: "/home/kp/mower/data/synthetic"
```

**Acceptance**: Configuration files created and validated

---

### BEAD-212: Integration Testing
**Status**: pending
**Priority**: high
**Depends**: BEAD-210, BEAD-201
**Description**: End-to-end testing of training pipeline

Create `scripts/test_integration.py`:
```python
"""Integration tests for training pipeline."""
import asyncio
import subprocess
from pathlib import Path

async def test_data_sync():
    """Test data sync from Pi."""
    result = subprocess.run(
        ["python", "scripts/sync_data.py"],
        capture_output=True,
        text=True
    )
    assert result.returncode == 0, f"Data sync failed: {result.stderr}"
    print("✓ Data sync test passed")

async def test_simulation():
    """Test Isaac Sim environment."""
    from simulation.lawn_env import LawnSimEnvironment
    env = LawnSimEnvironment()
    obs = env.reset()
    assert "stereo_left" in obs
    print("✓ Simulation test passed")

async def test_model_forward():
    """Test VLA model forward pass."""
    import torch
    from models.vla.lawn_vla import create_lawn_vla

    model = create_lawn_vla()
    batch = {
        "stereo_left": torch.randn(2, 3, 540, 960),
        "stereo_right": torch.randn(2, 3, 540, 960),
        "pi_camera": torch.randn(2, 3, 720, 1280),
        "gps": torch.randn(2, 4),
        "imu": torch.randn(2, 9),
        "ultrasonic": torch.randn(2, 3),
    }
    outputs = model(**batch)
    assert outputs["steering"].shape == (2, 1)
    print("✓ Model forward test passed")

async def test_distillation():
    """Test model distillation."""
    import torch
    from scripts.distill_hailo import DistilledLawnVLA

    model = DistilledLawnVLA()
    image = torch.randn(1, 3, 224, 224)
    sensors = torch.randn(1, 16)
    actions = model(image, sensors)
    assert actions.shape == (1, 3)
    print("✓ Distillation test passed")

async def main():
    print("Running integration tests...")
    await test_data_sync()
    await test_simulation()
    await test_model_forward()
    await test_distillation()
    print("\n✅ All integration tests passed!")

if __name__ == "__main__":
    asyncio.run(main())
```

**Acceptance**: All integration tests pass

---

## Progress Tracking

### Completed Tasks
(Update this section as tasks are completed)

### Known Issues
(Document any issues encountered)

### Notes
- JetPack 7 CUDA 13 compatibility notes go here
- Successful installation commands
- Performance benchmarks
